## **Epic 2: The Core Creative Engine (SSE Architecture)**

**Goal:** To implement the primary, server-driven user workflow for generating image variations in real-time using Server-Sent Events (SSE). This epic connects the UI to a backend that orchestrates all edits and streams the results back to you.

**Rationale:** This is the core "magic" of the application. By leveraging a real-time, server-driven architecture, we provide immediate visual feedback for a complex process. Completing this epic delivers the functional MVP and validates the entire concept.

---

### **User Story 2.1: The SSE Generation Endpoint**

* **Story:** As a **developer**, I need a robust SSE endpoint that can handle both initial image generation and variation generation from an existing image, so that we have a single entry point for all creative tasks.
* **Dependencies:** Story 1.1
* **Acceptance Criteria (AC):**
    * ✅ A new API route is created at `/api/generate-variations`.
    * ✅ The endpoint is configured to handle `POST` requests and maintain an open connection for an SSE stream (`Content-Type: text/event-stream`).
    * ✅ **Base Case (No Image):** If the request contains only a `prompt`, the endpoint generates a single base image using a Google tool and streams it back before closing the connection.
    * ✅ **Variation Case (With Image):** If the request contains a `prompt` and an `image` file (multipart/form-data), the endpoint initiates the 10-variation generation process.
    * ✅ The endpoint includes robust error handling to gracefully close the stream if an issue occurs.

---

### **User Story 2.2: Server-Side Execution Engine**

* **Story:** As a **developer**, I need a server-side engine that defines the edit plans and can apply each step to an image, so that all image processing is handled reliably on the backend.
* **Dependencies:** Story 2.1
* **Acceptance Criteria (AC):**
    * ✅ A file on the server (`/lib/plans.ts`) defines 10 fixed, hardcoded `Plan` arrays.
    * ✅ A core function, `applyStep(imageBuffer, step)`, is created.
    * ✅ This function can process all *local* edit `op`s (brightness, contrast, etc.) on an image buffer.
    * ✅ This function can also handle the `googleEdit` `op` by calling the appropriate Google AI service.
    * ✅ The `/api/generate-variations` endpoint uses this engine to iterate through the 10 plans, applying each step sequentially.
* **Technical Notes:**
    * Using a server-side image processing library like **Sharp** is highly recommended for performance.
    * This story centralizes all the image manipulation logic, making it consistent and scalable.



---

### **User Story 2.3: Streaming Plans & Progress via SSE**

* **Story:** As a **user**, I need the generation process to be transparent, with clear information streamed from the server, so I know what to expect and can track the results.
* **Dependencies:** Story 2.2
* **Acceptance Criteria (AC):**
    * ✅ The **first event** sent from the SSE stream is a `plans` event. Its data is a JSON object mapping a unique `variationId` (e.g., a UUID) to each of the 10 plans.
    * ✅ After each image editing step is completed on the server, a `step-result` event is sent.
    * ✅ The `step-result` event data includes the `variationId`, the `stepIndex`, and the resulting image encoded as a **Base64 string**.
    * ✅ After all 10 variations are complete, a final `end` event is sent with a success message.
* **Technical Notes:**
    * This event structure is critical. The initial `plans` event allows the frontend to set up all the "pending" nodes at once. The subsequent `step-result` events provide the real-time image updates.

---

### **User Story 2.4: Frontend SSE Client & State Management**

* **Story:** As a **user**, I want the graph to update in real-time as new images are generated by the server, so I can see my variations come to life.
* **Dependencies:** Epic 1, Story 2.3
* **Acceptance Criteria (AC):**
    * ✅ When you submit the `PromptBox`, the frontend initiates a `fetch` request to `/api/generate-variations`.
    * ✅ **Base Case:** If no node is selected, only the prompt is sent. A single new `ImageNode` is added to the graph and updated by the streamed image.
    * ✅ **Variation Case:** If a node is selected, its image is retrieved from Dexie and sent as a file in a `FormData` object along with the prompt.
    * ✅ The client correctly listens for the `plans` event and uses it to add 10 "pending" child nodes to the Zustand store, each tagged with its `variationId`.
    * ✅ The client listens for `step-result` events, finds the correct node in the store using the `variationId`, and updates its image with the new Base64 data.
    * ✅ The connection is gracefully closed when the `end` event is received.
* **Technical Notes:**
    * The browser's native **`EventSource`** API is perfect for handling the SSE connection on the client side.

---

### **User Story 2.5: Live Execution Feedback (Steps Bar)**

* **Story:** As a **user**, I want the "Steps Bar" to reflect the real-time server-side progress for a variation, so I can see exactly which edit is being applied.
* **Dependencies:** Story 1.4, Story 2.4
* **Acceptance Criteria (AC):**
    * ✅ The Zustand store is updated to track the progress of each `variationId`.
    * ✅ The initial `plans` event is used to populate the `StepsBar` with the full sequence of steps for one or more variations.
    * ✅ Each `step-result` event updates the UI, highlighting the step that was just completed.
    * ✅ The `StepsBar` can either track a single "focused" variation or show an overview of all 10.
* **Technical Notes:**
    * This story is now purely about reacting to state changes. The `StepsBar` component doesn't need to know about SSE; it only needs to know about the execution progress being reported in the Zustand store.


EXAMPLE GOOGLE IMAGE EDIT CODE
import { NextResponse } from 'next/server';
import { ChatGoogleGenerativeAI } from '@langchain/google-genai';

// Model configured for image editing/generation preview
const MODEL_NAME = process.env.GOOGLE_IMAGE_MODEL || 'gemini-2.5-flash-image-preview';

const llm = new ChatGoogleGenerativeAI({
  model: MODEL_NAME,
  temperature: 0,
});

function toUserMessage(prompt, imageDataUrl) {
  // LangChain message with text + inline base64 image
  return {
    role: 'user',
    content: [
      { type: 'text', text: prompt || '' },
      imageDataUrl ? { type: 'image_url', image_url: imageDataUrl } : null,
    ].filter(Boolean),
  };
}

function extractImageDataUrl(aiMessage) {
  const blocks = Array.isArray(aiMessage?.content) ? aiMessage.content : [];
  for (const b of blocks) {
    if (b && typeof b === 'object') {
      if (b.image_url && (typeof b.image_url === 'string' || typeof b.image_url?.url === 'string')) {
        const url = typeof b.image_url === 'string' ? b.image_url : b.image_url.url;
        if (typeof url === 'string' && url.startsWith('data:image/')) {
          return url;
        }
      }
      // Some providers may return inlineData:{ data, mimeType }
      if (b.inline_data || b.inlineData) {
        const inline = b.inline_data || b.inlineData;
        const mime = inline.mimeType || 'image/png';
        if (inline.data) {
          return `data:${mime};base64,${inline.data}`;
        }
      }
    }
  }
  return null;
}

export async function POST(request) {
  try {
    const { prompt, imageDataUrl } = await request.json();
    if (!prompt || !imageDataUrl) {
      return NextResponse.json(
        { success: false, error: 'Missing prompt or image data' },
        { status: 400 }
      );
    }

    // Build messages array. Keep simple one-turn for now.
    const messages = [toUserMessage(prompt, imageDataUrl)];

    // Ask for both text and image in response if supported
    const aiMessage = await llm.invoke(messages, {
      // Some SDKs accept generationConfig; LangChain forwards extras
      // Not all backends support this, so it's best-effort.
      response_modalities: ['TEXT', 'IMAGE'],
    });

    const editedDataUrl = extractImageDataUrl(aiMessage);
    if (!editedDataUrl) {
      return NextResponse.json(
        { success: false, error: 'No image returned by model' },
        { status: 502 }
      );
    }

    return NextResponse.json({ success: true, imageDataUrl: editedDataUrl });
  } catch (error) {
    console.error('Google image edit error:', error);
    let message = 'Internal server error';
    if (error.message?.toLowerCase().includes('api') || error.message?.toLowerCase().includes('key')) {
      message = 'Authentication failed. Check GOOGLE_API_KEY.';
    }
    return NextResponse.json({ success: false, error: message }, { status: 500 });
  }
}
---
